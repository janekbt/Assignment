---
title: "Assignment"
author: "Janek Teders"
date: "December 8, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, error = TRUE, cache = TRUE)
library(knitr)
library(readxl)
library(tidyverse)
set.seed(20190108)
load("0_data/Variables_from_Model_Answers.RData")
```

# 1. Regarding the DBC

## 1.1 Load the DBC codes and its descriptions

```{r}
dat <- read_xlsx("0_data/Tarievenoverzicht_HA1.xlsx", sheet = 1) %>%
  select(Code, `Omschrijving type 1`) %>%
  mutate(Code = as.numeric(Code))

all.equal(dat, Dutch_DBC)
```


### 1.2

```{r message=FALSE}
dict_dutch <- read_csv("0_data/the_Dutch_words.txt",
  col_names = F,
  trim_ws = F,
  na = character()
)

dict_english <- read_csv("0_data/the_English_translation.txt",
  col_names = F,
  trim_ws = F,
  na = character()
)

dict <- set_names(unlist(dict_english), unlist(dict_dutch))

dat_1_2 <- dat %>%
  mutate(
    `DBC Description 1` = str_replace_all(dat$`Omschrijving type 1`, dict)
  ) %>%
  select(Code, `DBC Description 1`)

all.equal(dat_1_2, DBC_tibble)
```



# 2. Regarding NZa fees only
## 2.1 Extract the NZa fees

```{r}
extract_NZa_fee <- function(path = "0_data/Tarievenoverzicht_HA1.xlsx") {
  path %>%
    excel_sheets() %>%
    grep("NZA", ., value = T) %>%
    set_names() %>%
    map_dfr(
      ~ read_excel(path = path, sheet = .x)[, c(1, 3)],
      .id = "NZA/year"
    ) %>%
    separate("NZA/year", into = c("NZA", "Year")) %>%
    transmute(
      Code = as.double(code),
      NZa_fee = coalesce(`basis landelijk`, tarief),
      Year = as.double(Year)
    )
}

all.equal(extract_NZa_fee(), NZa_fees)
```



## 2.2 A methodology change in computing the NZa fees.
### 2.2a

```{r message=FALSE}
dat_2_2a <- extract_NZa_fee()

NZa_fee_2014 <- dat_2_2a %>%
  filter(Year == 2014) %>%
  rename(NZA_2014 = NZa_fee) %>%
  select(NZA_2014, Code)

dat_2_2a %>%
  inner_join(., NZa_fee_2014, key = Code) %>%
  filter(Year != 2014) %>%
  na.omit() %>%
  group_by(Year) %>%
  summarise(
    `Min. change in %` = min(100 * (NZa_fee / NZA_2014 - 1)),
    `Average change in %` = mean(100 * (NZa_fee / NZA_2014 - 1)),
    `Median change in %` = median(100 * (NZa_fee / NZA_2014 - 1)),
    `Max. change in %` = max(100 * (NZa_fee / NZA_2014 - 1))
  ) %>%
  round(4) %>%
  kable(caption = "Changes in NZa fee computation compared to 2014")
```

### 2.2b 

```{r message=FALSE, warning=FALSE}
data_2_2 <- dat_2_2a %>%
  arrange(Code, Year) %>%
  group_by(Code) %>%
  mutate(
    diff = 100 * (NZa_fee / lag(NZa_fee) - 1),
    Period = paste(lag(Year), Year, sep = "-")
  ) %>%
  ungroup() %>%
  mutate(Year = factor(Year)) %>%
  filter(Year != 2014) %>%
  na.omit()

ggplot(data_2_2, aes(x = Period, y = diff)) +
  stat_boxplot(geom ='errorbar') +
  geom_boxplot() +
  labs(y = "% Increase of NZa fees")

```



## 2.3 Statistics and the Methodology for Computing the NZa Fees.
### The Programming Task
```{r message=FALSE}
d <- dat_2_2a %>%
  filter(Year %in% 2016:2018) %>%
  spread(key = Year, value = NZa_fee)

B <- 1000
n <- nrow(d)
result <- numeric(B)
perm <- replicate(B, sample(1:n)) %>% matrix(., ncol = B)
beta_hat <- mean(d$`2017` / d$`2016`) - 1
Tj_obs <- var(
  (d$`2017` - (1 + beta_hat) * d$`2016`) / d$`2016` * sqrt(d$`2016` / min(d$`2016`))
)
for (i in 1:B) {
  idx <- perm[, i]
  h0_perm <- d$`2016` * (d$`2017`[idx] / d$`2016`[idx])
  beta_hat <- mean(h0_perm / d$`2016`) - 1
  result[i] <- var(
    (h0_perm - (1 + beta_hat) * d$`2016`) / d$`2016` * sqrt(d$`2016` / min(d$`2016`))
  )
}

p_value <- mean(result > Tj_obs)

ggplot(as.tibble(result), aes(x = result)) +
  geom_histogram(binwidth = 0.01, fill = "grey", color = "black") +
  geom_vline(xintercept = Tj_obs, col = "red") +
  labs(x = "Tj", title = "2016-2017")
```

The p-value is `r p_value`. Considering an significance level of $\alpha = 0.05/2$ our result is highly significant and we can reject the null hypothesis.

```{r message=FALSE}
B <- 1000
n <- nrow(d)
result <- numeric(B)
perm <- replicate(B, sample(1:n)) %>% matrix(., ncol = B)
beta_hat <- mean(d$`2018` / d$`2017`) - 1
Tj_obs <- var(
  (d$`2018` - (1 + beta_hat) * d$`2017`) / d$`2017` * sqrt(d$`2017` / min(d$`2017`))
)
for (i in 1:B) {
  idx <- perm[, i]
  h0_perm <- d$`2017` * (d$`2018`[idx] / d$`2017`[idx])
  beta_hat <- mean(h0_perm / d$`2017`) - 1
  result[i] <- var(
    (h0_perm - (1 + beta_hat) * d$`2017`) / d$`2017` * sqrt(d$`2017` / min(d$`2017`))
  )
}
p_value_2 <- mean(result > Tj_obs)

ggplot(as.tibble(result), aes(x = result)) +
  geom_histogram(fill = "grey", color = "black") +
  geom_vline(xintercept = Tj_obs, col = "red") +
  labs(x = "Tj", title = "2017-2018")
```

The p-value is `r p_value_2`. Considering an significance level of $\alpha = 0.05/2$ our result is highly insignificant and we can't reject the null hypothesis.

# 3. About the Insurances
## 3.1 Creating the Data Set

```{r}
avrg_contr_fee <- function(path = "0_data/Tarievenoverzicht_HA1.xlsx") {
  insurances <- c("Z&Z", "ZK", "VGZ", "Menzis", "CZ")
  path %>%
    excel_sheets() %>%
    .[grepl(paste(insurances, collapse = "|"), .)] %>%
    set_names() %>%
    map_dfr(
      ~ read_xlsx(path = path, sheet = .x, col_types = "text"),
      .id = "id"
    ) %>%
    separate(id, c("Insurance", "Year"), sep = " ", remove = T) %>%
    transmute(
      Code = as.double(code),
      Fee = as.double(coalesce(`Gecontracteerde tarieven`, `1`)),
      Year = as.double(Year),
      Insurance = factor(Insurance, levels = insurances)
    )
}

all.equal(avrg_contr_fee(), fees_insurances)
```

## 3.2 Joining the NZa fees, the Insurance fees and the DBC codes with descriptions

```{r message=FALSE}
dat_2_1 <- extract_NZa_fee()
dat_3_1 <- avrg_contr_fee()

dat_NZa <- dat_2_1 %>%
  filter(Code %in% dat_1_2$Code) %>%
  mutate(Year = paste("NZa", Year)) %>%
  spread(key = Year, value = NZa_fee)

dat_3_2 <- dat_3_1 %>%
  filter(Code %in% dat_1_2$Code) %>%
  unite(insurance_year, Insurance, Year, sep = " ") %>%
  spread(key = insurance_year, value = Fee) %>%
  left_join(., dat_NZa, key = Code) %>%
  left_join(., dat_1_2, key = Code) %>%
  select(Code, `DBC Description 1`, everything())

all.equal(dat_3_2, all_fees)
```


### 3.3 Visualizing the Reimbursements for Non-Contracted Healh Care 

```{r message=FALSE, warning=FALSE}
insurance_levels <- c("Z&Z", "ZK", "VGZ", "Menzis", "CZ")
insurance_colors <- c(
  CZ = "orange",
  Menzis = "blue4",
  VGZ = "green4",
  `Z&Z` = "cornflowerblue",
  ZK = "darkred"
)

dat_fig_2 <- dat_3_2 %>%
  gather(key = "insurance_year", value = "fee", -Code, -`DBC Description 1`) %>%
  separate(insurance_year, c("Insurance", "Year"), sep = " ", remove = T) %>%
  filter(Insurance != "NZa") %>%
  mutate(Year = as.double(Year)) %>%
  left_join(., dat_2_1) %>%
  mutate(
    fee_0.75 = 75 * fee / NZa_fee,
    Insurance = factor(Insurance, levels = insurance_levels)
  )

ggplot(dat_fig_2, aes(x = Code, y = fee_0.75)) +
  geom_point(aes(color = Insurance)) +
  facet_grid(cols = vars(Year)) +
  scale_x_continuous(breaks = c(7, 100, 200, 300)) +
  ylim(50, 80) +
  scale_color_manual(values = insurance_colors) +
  labs(y = "% of NZa Fee Reimbursed") +
  theme(legend.title = element_blank())
```

## 3.4 About the Methodology of Average Contracted Fees
### 3.4a Average Contracted Fees

```{r}
dat_fig_3 <- dat_fig_2 %>%
  select(-`DBC Description 1`, -NZa_fee, -fee_0.75) %>%
  filter(Insurance != "NZa") %>%
  arrange(Code, Insurance) %>%
  group_by(Insurance, Code) %>%
  mutate(
    diff = 100 * (fee / lag(fee) - 1),
    Period = paste(lag(Year), Year, sep = "-")
  ) %>%
  ungroup() %>%
  mutate(Insurance = factor(Insurance, levels = insurance_levels)) %>%
  na.omit()

ggplot(dat_fig_3, aes(x = Insurance, y = diff, color = Insurance)) +
  stat_boxplot(geom = "errorbar") +
  geom_boxplot() +
  scale_color_manual(values = insurance_colors) +
  facet_grid(rows = vars(Period)) +
  labs(y = "Fee Increase in %") +
  theme(
    legend.title = element_blank(),
    axis.text.x = element_text(angle = 60, hjust = 1)
  )
```

For VGZ the in- or decrease of its fees is inside every interval up until the last one constant, which means they used around the same percentage of change for every code in one particular interval. The interval from 2017-2018 breaks with that rule, GVZ used multiple different percentages of change for its codes.

### 3.4b 

```{r}
dat_3_2 %>%
  select(matches("VGZ"), `NZa 2014`) %>%
  na.omit %>%
  cor(., use = "pairwise.complete.obs") %>%
  round(4) %>%
  kable(caption = "Correlation matrix of VGZ 2014-2018 and NZa 2014")
```

The correlation from VGZ 2014 until 2017 with NZa 2014 is pretty constant and close to 1. Only the correlation of VGZ 2018 and NZa 2014 differs slightly and is even closer to 1.

### 3.4c

```{r}
dat_3_2 %>%
  select(matches("NZa"), `Z&Z 2017`, `Z&Z 2018`) %>%
  na.omit %>%
  cor(., use = "pairwise.complete.obs") %>%
  round(4) %>%
  kable(caption = "Correlation matrix of NZa 2014-2018 and Z&Z 2017-2018")
```

This correlation between the Z&Z 2017 as well as 2018 and all the NZa fees between 2014 and 2018 exceed 0.99. Even though there is some variation one could say there is a well-neigh linear relationship.

```{r}
obs <- nrow(dat_3_2)
NAs <- sum(is.na(dat_3_2$`NZa 2015`))
```

I also wouldn't trust a correlation between NZa 2015 and any of the other Insurances because  for NZa 2015 out of `r obs` obesrvations `r NAs` are NAs.


## 3.5 Statistics on the % NZa fee reimbursed?
### 3.5a Boostrapped quantile intverals for VGZ and ZK?

```{r message=FALSE, warning=FALSE}
boot_strp_mean <- function(insurance){
  B <- 1000
  n <- 120
  d <- dat_fig_2 %>% filter(Insurance == insurance & Year == 2018)
  result <- sample(1:nrow(d), n*B, replace = T) %>% # Create samples
    matrix(., ncol = B) %>% # Puts them into a matrix
    d$`fee_0.75`[.] %>% # Uses the matrix to index the data
    matrix(., ncol = B) %>% # Converts the resulting vector into a matrix
    colMeans(., na.rm = TRUE) %>% 
    quantile(., probs = c(0.025, 0.975))
  return(result)
}

c(boot_strp_mean("VGZ"), boot_strp_mean("ZK")) %>%
  matrix(
    ncol = 2,
    dimnames = list(
      c("Lower bound: 2.5%", "Upper bound: 97.5%"),
      c("VGZ", c("ZK"))
    )
  ) %>%
  kable(caption = "Bootstrapped confidence intervals")
```


### 3.5b Exploiting Symmetry $\theta$

```{r message=FALSE, warning=FALSE}
symmetry <- function(data, B = 1000){
  N <- length(data)
  theta <- mean(data)
  t_b <- numeric(B)
  S <- sample(c(-1, 1), B * N, replace = TRUE) %>%
    matrix(ncol = B)
  t_b <- colMeans(S * (data - theta) + theta)
  return(t_b)
}

dat_ZK <- dat_fig_2 %>%
  filter(Insurance == "ZK" & Year == 2018) %>%
  na.omit %>%
  select(fee_0.75) %>%
  unlist(.) 

p_value_ZK <- mean(symmetry(dat_ZK) >= 62.5)

dat_VGZ <- dat_fig_2 %>%
  filter(Insurance == "VGZ" & Year == 2018) %>%
  na.omit %>%
  select(fee_0.75) %>%
  unlist(.)

p_value_VGZ <- mean(symmetry(dat_VGZ) <= 62.5)
```


The permutation p-value of ZK (`r p_value_ZK`) is below the significance level of $\alpha = 0.05$. We therefore can reject the null hypothesis. The permutation p-value of VGZ (`r p_value_VGZ`) on the other hand is bigger than $0.05$. We therefore can't reject the null hypothesis.

# 4. A Small Monte-Carlo Study regarding symmetry about $\theta$
## 4.1 Type-I error

```{r}
B_mc <- 1000
n <- length(dat_VGZ)
mu <- mean(dat_VGZ)
std <- 4.1
result <- numeric(B_mc)

for (i in 1:B_mc) {
  d <- rnorm(n, mu, std)
  result[i] <- mean(symmetry(d) <= 62.5)
}

type_1_err <- mean(result <= 0.05)
```
The type-I error is `r type_1_err`.

## 4.2 Power: More about symmetry about $\theta$

```{r}
B_mc <- 1000
n <- length(dat_VGZ)
mu <- 63.5
std <- 4.1
result <- numeric(B_mc)

for (i in 1:B_mc) {
  d <- rnorm(n, mu, std)
  result[i] <- mean(symmetry(d) <= 62.5)
}

power <- mean(result <= 0.05)
```

The power would be `r power`.

## 4.3 Reflecting on Permutation Tests.
### 4.3a

One strategy would be a Monte carlo Simulation on the computation of the power. By doing this we could construct a confidence interval which would be an estimate of the precision. Another method would be do again a Monte Carlo simulation as in the strategy before, but this time calculate the standard deviation or the variance as a measure of precision.

### 4.3b

Type-I error: the size is unaffected by B, it only affects the variance of multiple calculations of that error (higher power = smaller variance). A higher N increases the type-I error and vice versa.

Power: Same thing for the power concerning B and N as for the type-I error. 

